'''Workflow for the CAMP short-read quality control module.'''


from contextlib import redirect_stderr
from os import makedirs
from os.path import basename, exists, join
import pandas as pd
import shutil
from utils import Workflow_Dirs, ingest_samples, calc_read_lens, sample_statistics

from itertools import product

# Load and/or make the working directory structure
dirs = Workflow_Dirs(config['work_dir'], 'short_read_qc')


# Load sample names and input files 
SAMPLES = ingest_samples(config['samples'], dirs.TMP)


# Optional rules management

## 1) Host read filtering and error correction

STEPS = ['0_lowqual_removal', '1_adapter_removal']
if config['use_host_filter']:
    STEPS.append('2_host_removal')
if config['error_correction'] == 'bayeshammer':
    STEPS.append('3_error_removal/bayeshammer')
    d = join(dirs.OUT, '3_error_removal/bayeshammer')
else:
    STEPS.append('3_error_removal/tadpole')
    d = join(dirs.OUT, '3_error_removal/tadpole')
if not exists(d):
    makedirs(d)

def workflow_mode_reads(wildcards):
    if config['use_host_filter']:
        return [ join(dirs.OUT, '2_host_removal', '{sample}_1.fastq.gz'),
                 join(dirs.OUT, '2_host_removal', '{sample}_2.fastq.gz') ]
    else:
        return [ join(dirs.OUT, '1_adapter_removal', '{sample}_1.fastq.gz'),
                 join(dirs.OUT, '1_adapter_removal', '{sample}_2.fastq.gz') ]

## 2) Error correction (controls FastQC)

def workflow_mode_err(wildcards):
    if config['error_correction'] == 'bayeshammer':
        return join(dirs.OUT, '3_error_removal', 'bayeshammer', '{sample}_{dir}.fastq.gz')
    else:
        return join(dirs.OUT, '3_error_removal', 'tadpole', '{sample}_{dir}.fastq.gz')

## 3) Dataviz with FastQC/MultiQC

def workflow_mode_viz(wildcards):
    results = [join(dirs.OUT, 'final_reports', 'read_stats.csv')]
    if config['qc_dataviz']:
        results.extend([join(dirs.OUT, '4_summary', 'pre_multiqc_report.html'), join(dirs.OUT, '4_summary', 'post_multiqc_report.html')])
    return results


# --- Workflow output --- #


rule all:
    input:
        join(dirs.OUT, 'final_reports', 'samples.csv'),


# --- Workflow modules --- #

# low quality sequence filter using fastp
# Shifu Chen, Yanqing Zhou, Yaru Chen, Jia Gu; fastp: an ultra-fast all-in-one FASTQ preprocessor, Bioinformatics, Volume 34, Issue 17, 1 September 2018, Pages i884â€“i890, 
# https://doi.org/10.1093/bioinformatics/bty560
# Multi-purpose filter: Quality, length, Ns, polyG/X
# No deduplication to keep relative abundance signal
rule filter_low_qual:
    input:
        fwd = join(dirs.TMP,'{sample}_1.fastq.gz'),
        rev = join(dirs.TMP,'{sample}_2.fastq.gz'),
    output:
        fwd = join(dirs.OUT, '0_lowqual_removal', '{sample}_1.fastq.gz'),
        rev = join(dirs.OUT, '0_lowqual_removal', '{sample}_2.fastq.gz'),
    log:
        join(dirs.LOG, 'lowqual_removal', '{sample}.out'),
    conda: "fastp"
    threads: config['filter_lowqual_threads'],
    resources:
        mem_mb = config['filter_lowqual_mem_mb'],
    params:
        minqual = config['minqual'],
        dedup = '--dedup' if config['dedup'] else '--dont_eval_duplication',
        sample = '{sample}',
        out_dir = join(dirs.OUT, '0_lowqual_removal'),
    shell:
        """
        fastp -i {input.fwd} -I {input.rev} -o {output.fwd} -O {output.rev} \
            -q {params.minqual} {params.dedup} --thread {threads} \
            -j {params.out_dir}/{params.sample}.json \
            -h {params.out_dir}/{params.sample}.html > {log} 2>&1
        """


# filter adapters using AdapterRemoval
# Schubert, Lindgreen, and Orlando (2016). AdapterRemoval v2: rapid adapter trimming, identification, and read merging. BMC Research Notes, 12;9(1):88 
# http://bmcresnotes.biomedcentral.com/articles/10.1186/s13104-016-1900-2
rule filter_adapters:
    input:
        fwd = join(dirs.OUT, '0_lowqual_removal', '{sample}_1.fastq.gz'),
        rev = join(dirs.OUT, '0_lowqual_removal', '{sample}_2.fastq.gz'),
    output:
        fwd  = join(dirs.OUT, '1_adapter_removal', '{sample}_1.fastq.gz'),
        rev  = join(dirs.OUT, '1_adapter_removal', '{sample}_2.fastq.gz'),
    conda:
        "adapterremoval"
    threads: config['filter_adapters_threads'],
    params:
        adapt_lst = '--adapter-list ' + str(config['adapters']),
        prefix = join(dirs.OUT, '1_adapter_removal', '{sample}'),
    shell:
        """
        AdapterRemoval --gzip --file1 {input.fwd} --file2 {input.rev} \
            --output1 {output.fwd} --output2 {output.rev} \
            --discarded {params.prefix}.discarded.fastq.gz \
            --singleton {params.prefix}.singleton.fastq.gz \
            --settings {params.prefix}.settings \
            {params.adapt_lst} --trimns --trimqualities --threads {threads}
        """


# [OPTIONAL] filtering out host reads using bowtie2
# Langmead B, Salzberg SL. Fast gapped-read alignment with Bowtie 2. Nat Methods. 2012 Mar 4;9(4):357-9. doi: 10.1038/nmeth.1923. PMID: 22388286; PMCID: PMC3322381.
# https://www.metagenomics.wiki/tools/short-read/remove-host-sequences
rule filter_host_reads:
    input:
        fwd = join(dirs.OUT, '1_adapter_removal', '{sample}_1.fastq.gz'),
        rev = join(dirs.OUT, '1_adapter_removal', '{sample}_2.fastq.gz')
    output:
        fwd = join(dirs.OUT, '2_host_removal', '{sample}_1.fastq.gz'),
        rev = join(dirs.OUT, '2_host_removal', '{sample}_2.fastq.gz')
    log:
        join(dirs.LOG, 'host_removal', '{sample}.out'),
    threads: config['filter_host_reads_threads'],
    resources:
        mem_mb = config['filter_host_reads_mem_mb'],
    params:
        prefix = join(dirs.OUT, '2_host_removal', '{sample}'),
        host_ref = config['host_ref_genome'],
    shell:
        """
        bowtie2 --very-sensitive --threads {threads} -x {params.host_ref} \
            --un-conc-gz {params.prefix}_%.fastq.gz \
            -1 {input.fwd} -2 {input.rev} > {params.prefix}.sam 2> {log}
        rm {params.prefix}.sam
        """


# filter out sequencing errors using either spades' error correction module (BayesHammer) or tadpole
# BayesHammer: Nikolenko, S.I., Korobeynikov, A.I. & Alekseyev, M.A. BayesHammer: Bayesian clustering for error correction in single-cell sequencing. BMC Genomics 14 (Suppl 1), S7 (2013). https://doi.org/10.1186/1471-2164-14-S1-S7
rule filter_seq_errors_bh:
    input:
        workflow_mode_reads,
    output:
        fwd = join(dirs.OUT, '3_error_removal', 'bayeshammer', '{sample}', 'corrected', '{sample}_1.fastq.00.0_0.cor.fastq.gz'),
        rev = join(dirs.OUT, '3_error_removal', 'bayeshammer', '{sample}', 'corrected', '{sample}_2.fastq.00.0_0.cor.fastq.gz'),
    log:
        join(dirs.LOG, 'error_removal', '{sample}.out')
    conda: "spades"
    threads: config['filter_seq_errors_threads'],
    resources:
        mem_mb = config['filter_seq_errors_mem_mb'],
    params:
        prefix = join(dirs.OUT, '3_error_removal', 'bayeshammer', '{sample}', 'corrected', '{sample}'),
        out_dir = join(dirs.OUT, '3_error_removal', 'bayeshammer', '{sample}', ),
    shell:
        """
        spades.py --only-error-correction --meta \
            -t {threads} -m {resources.mem_mb} \
            -1 {input[0]} -2 {input[1]} -o {params.out_dir} > {log} 2>&1
        """


# procedural step so that FastQs generated in the previous step will not get deleted
rule move_corr_reads: 
    input:
        fwd = join(dirs.OUT, '3_error_removal', 'bayeshammer', '{sample}', 'corrected', '{sample}_1.fastq.00.0_0.cor.fastq.gz'),
        rev = join(dirs.OUT, '3_error_removal', 'bayeshammer', '{sample}', 'corrected', '{sample}_2.fastq.00.0_0.cor.fastq.gz'),
    output:
        fwd = join(dirs.OUT, '3_error_removal', 'bayeshammer', '{sample}_1.fastq.gz'),
        rev = join(dirs.OUT, '3_error_removal', 'bayeshammer', '{sample}_2.fastq.gz'),
    params:
        inp_unp = join(dirs.OUT, '3_error_removal', 'bayeshammer', '{sample}', 'corrected', '{sample}__unpaired.00.0_0.cor.fastq.gz'),
        out_unp = join(dirs.OUT, '3_error_removal', 'bayeshammer', '{sample}_unp.fastq.gz'),
    shell:
        """      
        mv {input.fwd} {output.fwd}
        mv {input.rev} {output.rev}
        if [ -f {params.inp_unp} ]; then
            mv {params.inp_unp} {params.out_unp}
        fi
        touch {output}
        """


# tadpole (as part of BBMap): Bushnell, B. BBMap:  A Fast, Accurate, Splice-Aware Aligner. https://www.osti.gov/biblio/1241166
rule filter_seq_errors_tp:
    input:
        workflow_mode_reads,
    output:
        fwd = join(dirs.OUT, '3_error_removal', 'tadpole', '{sample}_1.fastq.gz'),
        rev = join(dirs.OUT, '3_error_removal', 'tadpole', '{sample}_2.fastq.gz'),
    conda: "bbmap"  
    log:
        join(dirs.LOG, 'error_removal', '{sample}.out')
    threads: config['filter_seq_errors_threads'],
    resources:
        mem_mb = config['filter_seq_errors_mem_mb'],
    params:
        fwd = join(dirs.OUT, '3_error_removal', 'tadpole', '{sample}_tmp_1.fastq.gz'),
        rev = join(dirs.OUT, '3_error_removal', 'tadpole', '{sample}_tmp_2.fastq.gz'),
    shell:
        """
        repair.sh in={input[0]} in2={input[1]} out={params.fwd} out2={params.rev}
        tadpole.sh mode=correct  t={threads} in={params.fwd} in2={params.rev} out={output.fwd} out2={output.rev}
        """


# [OPTIONAL] fastqc before everything to give quality control visualization and stats
# Andrews, S. (2010). FastQC:  A Quality Control Tool for High Throughput Sequence Data [Online]. Available online at: http://www.bioinformatics.babraham.ac.uk/projects/fastqc/
rule fastqc_pre:
    input:
        join(dirs.TMP,'{sample}_{dir}.fastq.gz'),
    output:
        join(dirs.OUT, '4_summary', 'fastqc_pre', '{sample}_{dir}_fastqc.html'),
    conda: "fastqc"
        #join(config['env_yamls'], 'multiqc.yaml'),	
    threads: config['qc_threads'],
    params:
        out_dir = join(dirs.OUT, '4_summary', 'fastqc_pre'),
    shell:
        """
        fastqc {input} -d {params.out_dir} --outdir {params.out_dir} -t {threads}
        """


# [OPTIONAL] fastqc after everything to give quality control visualization and stats
# Andrews, S. (2010). FastQC:  A Quality Control Tool for High Throughput Sequence Data [Online]. Available online at: http://www.bioinformatics.babraham.ac.uk/projects/fastqc/
rule fastqc_post:
    input:
        workflow_mode_err,
    output:
        join(dirs.OUT, '4_summary', 'fastqc_post', '{sample}_{dir}_fastqc.html'),
    conda: "fastqc"
    threads: config['qc_threads'],
    params:
        out_dir = join(dirs.OUT, '4_summary', 'fastqc_post'),
    shell:
        """
        fastqc {input} -d {params.out_dir} --outdir {params.out_dir} -t {threads}
        """


# [OPTIONAL] multiqc for multi-sequence quality control report generation
# Philip Ewels, MÃ¥ns Magnusson, Sverker Lundin, Max KÃ¤ller, MultiQC: summarize analysis results for multiple tools and samples in a single report, Bioinformatics, Volume 32, Issue 19, 1 October 2016, Pages 3047â€“3048, https://doi.org/10.1093/bioinformatics/btw354
rule multiqc:
    input:
        lambda wildcards: expand(join(dirs.OUT, '4_summary', 'fastqc_{eval}', '{sample}_{dir}_fastqc.html'), eval = wildcards.eval, sample = SAMPLES, dir = ['1', '2']),
    output:
        join(dirs.OUT, '4_summary', '{eval}_multiqc_report.html'),
    conda: "multiqc"
    threads: config['qc_threads'],
    params:
        in_dir = join(dirs.OUT, '4_summary', 'fastqc_{eval}'),
    shell:
        """
        multiqc --force {params.in_dir} -n {output}
        """


# generate initial data stats
rule init_statistics:
    input:
        fwd = join(dirs.TMP,'{sample}_1.fastq.gz'),
        rev = join(dirs.TMP,'{sample}_2.fastq.gz'),
    output:
        join(dirs.OUT, '{sample}' + '_read_stats.csv'),
    resources:
        mem_mb = config['count_reads_mem_mb'],
    params:
        sample = '{sample}',
    run:
        calc_read_lens(str(params.sample), '0_begin', input, str(output))


# generate data stats after each step
rule step_statistics:
    input:
        lambda wildcards: expand(join(dirs.OUT, '{step}', '{sample}_{dir}.fastq.gz'), step = wildcards.step, sample = wildcards.sample, dir = ['1', '2'])
    output:
        join(dirs.OUT, '{step}', '{sample}_read_stats.csv'),
    resources:
        mem_mb = config['count_reads_mem_mb'],
    params:
        sample = '{sample}',
        step = '{step}',
    run:
        calc_read_lens(str(params.sample), str(params.step), input, str(output))


# generate sample statistics after finishing all above rules
rule sample_statistics:
    input:
        init = join(dirs.OUT, '{sample}' + '_read_stats.csv'),
        step = lambda wildcards: expand(join(dirs.OUT, '{step}', '{sample}_read_stats.csv'), step = STEPS, sample = wildcards.sample),
    output:
        join(dirs.OUT, '4_summary', '{sample}_read_stats.csv'),
    run:
        sample_statistics([str(input.init)] + input.step, str(output))


# concatenate statistics after finishing all rules
rule concat_statistics:
    input:
        expand(join(dirs.OUT, '4_summary', '{sample}_read_stats.csv'), sample = SAMPLES),
    output:
        join(dirs.OUT, 'final_reports', 'read_stats.csv'),
    shell:
        """
        echo -e "sample_name,step,num_reads,prop_init_reads,total_size,prop_init_size,mean_read_len" | cat - {input} > {output}
        """


rule make_config:
    input:
        workflow_mode_viz,
    output:
        join(dirs.OUT, 'final_reports', 'samples.csv'),
    params: 
        fastq_dir = join(dirs.OUT, '3_error_removal'),
        samples = SAMPLES,
        pre = join(dirs.OUT, 'final_reports', 'pre_multiqc_report.html'),
        post = join(dirs.OUT, 'final_reports', 'post_multiqc_report.html'),
    run:
        if len(input) > 1:
            shutil.copy(str(input[1]), str(params.pre))
            shutil.copy(str(input[2]), str(params.post))
        dct = {}
        for i in params.samples:
            s = i.split('/')[-1]
            if s not in dct: dct[s] = {}
            dct[s]['illumina_fwd'] = join(params.fastq_dir, s + '_1.fastq.gz')
            dct[s]['illumina_rev'] = join(params.fastq_dir, s + '_2.fastq.gz')
        df = pd.DataFrame.from_dict(dct, orient ='index')
        df.reset_index(inplace = True)
        df.rename(columns = {'index': 'sample_name'}, inplace = True)
        df.to_csv(str(output), index = False)
