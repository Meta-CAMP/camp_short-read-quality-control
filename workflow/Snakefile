'''Workflow for the CAMP short-read quality control module.'''


from contextlib import redirect_stderr
import os
from os.path import basename, join
import pandas as pd
import shutil
from utils import Workflow_Dirs, ingest_samples, calc_read_lens, sample_statistics


# Load and/or make the working directory structure
dirs = Workflow_Dirs(config['work_dir'], 'short_read_qc')


# Load sample names and input files 
SAMPLES = ingest_samples(config['samples'], dirs.TMP)


# Configure optional flags
dedup_flag = '--dedup' if config['dedup'] else '--dont_eval_duplication'


# --- Workflow output --- #


rule all:
    input:
        expand(join(dirs.OUT, 'final_reports', '{eval}_multiqc_report.html'), eval = ['pre', 'post']),
        join(dirs.OUT, 'final_reports', 'read_stats.csv'),
        join(dirs.OUT, 'final_reports', 'samples.csv')


def workflow_mode(wildcards):
    if config['use_host_filter']:
        return [ join(dirs.OUT, '2_host_removal', '{sample}_1.fastq.gz'),
                 join(dirs.OUT, '2_host_removal', '{sample}_2.fastq.gz') ]
    else:
        return [ join(dirs.OUT, '1_adapter_removal', '{sample}_1.fastq.gz'),
                 join(dirs.OUT, '1_adapter_removal', '{sample}_2.fastq.gz') ]


# --- Workflow modules --- #


# Multi-purpose filter: Quality, length, Ns, polyG/X
# No deduplication to keep relative abundance signal
rule filter_low_qual:
    input:
        fwd = join(dirs.TMP,'{sample}_1.fastq.gz'),
        rev = join(dirs.TMP,'{sample}_2.fastq.gz'),
    output:
        fwd = join(dirs.OUT, '0_lowqual_removal', '{sample}_1.fastq.gz'),
        rev = join(dirs.OUT, '0_lowqual_removal', '{sample}_2.fastq.gz'),
    log:
        join(dirs.LOG, 'lowqual_removal', '{sample}.out'),
    threads: config['filter_lowqual_threads'],
    resources:
        mem_mb = config['filter_lowqual_mem_mb'],
    params:
        minqual = config['minqual'],
        dedup = dedup_flag,
        sample = '{sample}',
        out_dir = join(dirs.OUT, '0_lowqual_removal'),
    shell:
        """
        fastp -i {input.fwd} -I {input.rev} -o {output.fwd} -O {output.rev} \
            -q {params.minqual} {params.dedup} --thread {threads} \
            -j {params.out_dir}/{params.sample}.json \
            -h {params.out_dir}/{params.sample}.html > {log} 2>&1
        """


rule filter_adapters:
    input:
        fwd = join(dirs.OUT, '0_lowqual_removal', '{sample}_1.fastq.gz'),
        rev = join(dirs.OUT, '0_lowqual_removal', '{sample}_2.fastq.gz'),
    output:
        fwd = join(dirs.OUT, '1_adapter_removal', '{sample}_1.fastq.gz'),
        rev = join(dirs.OUT, '1_adapter_removal', '{sample}_2.fastq.gz'),
        fun = join(dirs.OUT, '1_adapter_removal', '{sample}_1_unp.fastq.gz'),
        run = join(dirs.OUT, '1_adapter_removal', '{sample}_2_unp.fastq.gz'),
    log:
        join(dirs.LOG, 'adapter_removal', '{sample}.out'),
    threads: config['filter_adapters_threads'],
    resources:
        mem_mb = config['filter_adapters_mem_mb'],
    params:
        trim_exec = config['trimmomatic_exec'],
        trim_adapters = config['adapters'],
        minqual = config['minqual'],
    shell:
        """
        java -classpath {params.trim_exec} org.usadellab.trimmomatic.TrimmomaticPE \
            -phred33 -threads {threads} -trimlog {log} \
            {input.fwd} {input.rev} \
            {output.fwd} {output.fun} {output.rev} {output.run} \
            ILLUMINACLIP:{params.trim_adapters}:2:{params.minqual}:10:3:true
        """
    

rule filter_host_reads:
    input:
        fwd = join(dirs.OUT, '1_adapter_removal', '{sample}_1.fastq.gz'),
        rev = join(dirs.OUT, '1_adapter_removal', '{sample}_2.fastq.gz')
    output:
        fwd = join(dirs.OUT, '2_host_removal', '{sample}_1.fastq.gz'),
        rev = join(dirs.OUT, '2_host_removal', '{sample}_2.fastq.gz')
    log:
        join(dirs.LOG, 'host_removal', '{sample}.out'),
    threads: config['filter_host_reads_threads'],
    resources:
        mem_mb = config['filter_host_reads_mem_mb'],
    params:
        prefix = join(dirs.OUT, '2_host_removal', '{sample}'),
        host_ref_db = config['host_reference_database'],
    shell:
        """
        bowtie2 --very-sensitive --threads {threads} -x {params.host_ref_db} \
            -1 {input.fwd} -2 {input.rev} --un-conc-gz {params.prefix}_%.fastq.gz \
            > {params.prefix}.sam 2> {log}
        rm {params.prefix}.sam
        """


rule filter_seq_errors:
    input:
        workflow_mode,
    output:
        fwd = join(dirs.OUT, '3_error_removal', '{sample}', 'corrected', '{sample}_1.fastq.00.0_0.cor.fastq.gz'),
        rev = join(dirs.OUT, '3_error_removal', '{sample}', 'corrected', '{sample}_2.fastq.00.0_0.cor.fastq.gz'),
    log:
        join(dirs.LOG, 'error_removal', '{sample}.out')
    threads: config['filter_seq_errors_threads'],
    resources:
        mem_mb = config['filter_seq_errors_mem_mb'],
    params:
        prefix = join(dirs.OUT, '3_error_removal', '{sample}', 'corrected', '{sample}'),
        out_dir = join(dirs.OUT, '3_error_removal', '{sample}', ),
    shell:
        """
        spades.py --only-error-correction --meta \
            -t {threads} -m {resources.mem_mb} \
            -1 {input[0]} -2 {input[1]} -o {params.out_dir} > {log} 2>&1
        """


# So that FastQs generated in the previous step will not get deleted
rule move_corr_reads: 
    input:
        fwd = join(dirs.OUT, '3_error_removal', '{sample}', 'corrected', '{sample}_1.fastq.00.0_0.cor.fastq.gz'),
        rev = join(dirs.OUT, '3_error_removal', '{sample}', 'corrected', '{sample}_2.fastq.00.0_0.cor.fastq.gz'),
    output:
        fwd = join(dirs.OUT, '3_error_removal', '{sample}_1.fastq.gz'),
        rev = join(dirs.OUT, '3_error_removal', '{sample}_2.fastq.gz'),
    params:
        inp_unp = join(dirs.OUT, '3_error_removal', '{sample}', 'corrected', '{sample}__unpaired.00.0_0.cor.fastq.gz'),
        out_unp = join(dirs.OUT, '3_error_removal', '{sample}_unp.fastq.gz'),
    shell:
        """      
        mv {input.fwd} {output.fwd}
        mv {input.rev} {output.rev}
        if [ -f {params.inp_unp} ]; then
            mv {params.inp_unp} {params.out_unp}
        fi
        """


rule fastqc_pre:
    input:
        join(dirs.TMP,'{sample}_{dir}.fastq.gz'),
    output:
        join(dirs.OUT, '4_summary', 'fastqc_pre', '{sample}_{dir}_fastqc.html'),
    conda:
        join(config['env_yamls'], 'multiqc.yaml'),
    params:
        out_dir = join(dirs.OUT, '4_summary', 'fastqc_pre'),
        temp_dir = dirs.OUT,
    shell:
        """
        fastqc {input} -d {params.temp_dir} --outdir {params.out_dir}
        """


rule fastqc_post:
    input:
        join(dirs.OUT, '3_error_removal', '{sample}_{dir}.fastq.gz'),
    output:
        join(dirs.OUT, '4_summary', 'fastqc_post', '{sample}_{dir}_fastqc.html'),
    conda:
        join(config['env_yamls'], 'multiqc.yaml'),
    params:
        out_dir = join(dirs.OUT, '4_summary', 'fastqc_post'),
    shell:
        """
        fastqc {input} --outdir {params.out_dir}
        """


rule multiqc:
    input:
        lambda wildcards: expand(join(dirs.OUT, '4_summary', 'fastqc_{eval}', '{sample}_{dir}_fastqc.html'), eval = wildcards.eval, sample = SAMPLES, dir = ['1', '2']),
    output:
        join(dirs.OUT, '4_summary', '{eval}_multiqc_report.html'),
    conda:
        join(config['env_yamls'], 'multiqc.yaml'),
    params:
        in_dir = join(dirs.OUT, '4_summary', 'fastqc_{eval}'),
    shell:
        """
        multiqc --force {params.in_dir} -n {output}
        """


rule init_statistics:
    input:
        fwd = join(dirs.TMP,'{sample}_1.fastq.gz'),
        rev = join(dirs.TMP,'{sample}_2.fastq.gz'),
    output:
        join(dirs.OUT, '{sample}' + '_read_stats.csv'),
    resources:
        mem_mb = config['count_reads_mem_mb'],
    params:
        sample = '{sample}',
    run:
        calc_read_lens(str(params.sample), '0_begin', input, str(output))


rule step_statistics:
    input:
        lambda wildcards: expand(join(dirs.OUT, '{step}', '{sample}_{dir}.fastq.gz'), step = wildcards.step, sample = wildcards.sample, dir = ['1', '2'])
    output:
        join(dirs.OUT, '{step}', '{sample}_read_stats.csv'),
    resources:
        mem_mb = config['count_reads_mem_mb'],
    params:
        sample = '{sample}',
        step = '{step}',
    run:
        calc_read_lens(str(params.sample), str(params.step), input, str(output))


rule sample_statistics:
    input:
        init = join(dirs.OUT, '{sample}' + '_read_stats.csv'),
        step = lambda wildcards: expand(join(dirs.OUT, '{step}', '{sample}_read_stats.csv'), step = ['0_lowqual_removal', '1_adapter_removal', '2_host_removal', '3_error_removal'], sample = wildcards.sample),
    output:
        join(dirs.OUT, '4_summary', '{sample}_read_stats.csv'),
    run:
        sample_statistics([str(input.init)] + input.step, str(output))


rule concat_statistics:
    input:
        expand(join(dirs.OUT, '4_summary', '{sample}_read_stats.csv'), sample = SAMPLES),
    output:
        join(dirs.OUT, 'final_reports', 'read_stats.csv'),
    shell:
        """
        echo -e "sample_name,step,num_reads,prop_init_reads,total_size,prop_init_size,mean_read_len" | cat - {input} > {output}
        """


rule make_config:
    input:
        mqc = expand(join(dirs.OUT, '4_summary', '{eval}_multiqc_report.html'), eval = ['pre', 'post']),
        sts = join(dirs.OUT, 'final_reports', 'read_stats.csv'),
    output:
        cfg = join(dirs.OUT, 'final_reports', 'samples.csv'),
        pre = join(dirs.OUT, 'final_reports', 'pre_multiqc_report.html'),
        post = join(dirs.OUT, 'final_reports', 'post_multiqc_report.html'),
    params: 
        fastq_dir = join(dirs.OUT, '3_error_removal'),
        multiqc_dir = join(dirs.OUT, '4_summary'),
        samples = SAMPLES,
    run:
        shutil.copy(str(input.mqc[0]), str(output.pre))
        shutil.copy(str(input.mqc[1]), str(output.post))
        dct = {}
        for i in params.samples:
            s = i.split('/')[-1]
            if s not in dct: dct[s] = {}
            dct[s]['illumina_fwd'] = join(params.fastq_dir, s + '_1.fastq.gz')
            dct[s]['illumina_rev'] = join(params.fastq_dir, s + '_2.fastq.gz')
        df = pd.DataFrame.from_dict(dct, orient ='index')
        df.reset_index(inplace = True)
        df.rename(columns = {'index': 'sample_name'}, inplace = True)
        df.to_csv(str(output.cfg), index = False)
